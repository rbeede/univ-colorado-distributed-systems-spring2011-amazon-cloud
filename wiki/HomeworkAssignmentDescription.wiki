#summary Description of homework assignment

<strong><a class="autolink" title="Programming Assignment - Distributed Systems" href="http://moodle.cs.colorado.edu/mod/assignment/view.php?id=252">Programming Assignment - Distributed Systems</a></strong>

<p style="text-align: center;"><span style="font-size: medium; font-family: arial,helvetica,sans-serif;"><strong>Due Friday March 4 by 11:55 pm</strong></span><span style="text-decoration: underline; font-size: medium; font-family: arial,helvetica,sans-serif;"><strong><br /></strong></span></p>

<p>The goal of this programming assignment is to enable you to gain experience programming with:</p>

<p>- Amazon Web Services, specifically the EC2 cloud, and S3</p>

<p>- the Hadoop open source framework</p>

<p>- and breaking down a task into a parallel distributed mapreduce model</p>

<p>You may form groups of two to complete this assignment.</p>

<p><strong>Sign up</strong> for an account on Amazon Web Services.  Use your Amazon key code for $100 credit.  Redeem it at <a class="moz-txt-link-freetext" href="http://aws.amazon.com/awscredits">http://aws.amazon.com/awscredits</a>.  Learn about EC2 at <a title="Amazon EC2" href="http://aws.amazon.com/ec2/">http://aws.amazon.com/ec2/</a>.</p>

<p><strong>Find a dataset</strong>.  There are some publicly available datasets on Amazon at <a title="Amazon data sets" href="http://aws.amazon.com/datasets">http://aws.amazon.com/datasets</a>.  For example, there's a global weather data set at <a title="Weather data set" href="http://aws.amazon.com/datasets/2759">http://aws.amazon.com/datasets/2759</a> (20 GB), the 2000 Census, transportation, economics, and Wikipedia page statistics (320 GB).  Wikipedia itself can be obtained at <a title="Wikipedia data sets" href="http://dumps.wikimedia.org/">http://dumps.wikimedia.org/</a>.  You don't have to use a complete data set.  Let's say 10-100 MB should be sufficient, though you're welcome to try GB of data.<span style="font-size: 32pt; font-family: &quot;Comic Sans MS&quot;; color: black;"> </span><span style="font-size: 32pt; font-family: &quot;Comic Sans MS&quot;; color: black;"></span></p>

<p><strong>Install Hadoop</strong> on your Amazon instance AMI.  Process this data set using the Hadoop framework on EC2.  See <a title="Hadoop Apache" href="http://hadoop.apache.org/">http://hadoop.apache.org/</a> for the source code.  You are not allowed to use Amazon's Elastic {{{MapReduce}}} service.  You will have to install hadoop on your own VM instances.  See an example of how to install Hadoop on AWS at <a title="Hadoop on EC2" href="http://www.cloudera.com/blog/2009/05/using-clouderas-hadoop-amis-to-process-ebs-datasets-on-ec2/">http://www.cloudera.com/blog/2009/05/using-clouderas-hadoop-amis-to-process-ebs-datasets-on-ec2/</a>.<span style="font-size: 24pt; font-family: &quot;Comic Sans MS&quot;; color: black;"></span></p>

<p><strong>Calculate</strong> the most frequent data item in this data set using Hadoop for a given category, say temperature for the weather data set, or the most popular Wikipedia page.  Sort the data set from most frequent item to least frequent item.  You should use at least 10 instances to perform the mapreduce calculations.  Feel free to vary the # of instances and type of instance to see how the performance varies.  Is it linear across number of instances for a given type?</p>

<p>Here's a mapreduce example <a title="Mapreduce example" href="http://hadoop.apache.org/common/docs/r0.20.2/mapred_tutorial.html#Example%3A+WordCount+v2.0">http://hadoop.apache.org/common/docs/r0.20.2/mapred_tutorial.html#Example%3A+WordCount+v2.0</a>.   Also see <a title="Google code mapreduce" href="http://code.google.com/edu/parallel/mapreduce-tutorial.html">http://code.google.com/edu/parallel/mapreduce-tutorial.html</a>.</p>

<p><strong>Store</strong> the results persistently in Amazon S3 or another persistent data store.  Visualize the sorted results in a suitable way, so it can be observed that the data items were properly sorted.</p>

<p><strong>Grading</strong>: upload a zip file of your source code.  We'll arrange grading meetings in early to mid March to go over the results of your Hadoop implementation on EC2.</p>

<p><strong>Extra Credit:</strong> use mapreduce to compute a histogram of your data frequencies.  I think binning would be helpful here.</p>

<p>Any additional updates will be posted here and/or via updates in the <a class="autolink" title="News forum" href="http://moodle.cs.colorado.edu/mod/forum/view.php?id=11">news forum</a>.</p><div id="dates" class="box generalbox boxaligncenter"><table><tr><td class="c0">Available from:</td>    <td class="c1">Saturday, January 29, 2011, 03:05 PM</td></tr><tr><td class="c0">Due date:</td>    <td class="c1">Friday, March 4, 2011, 11:55 PM</td></tr></table></div>